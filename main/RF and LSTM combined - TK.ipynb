{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fault prediction and classification for the Tennessee Eastman Process (TEP)**\n",
    "\n",
    "This Jupyter notebook presents a comprehensive exploration of machine learning models aimed at predicting and classifying faults within the Tennessee Eastman Process. The Tennessee Eastman Process is a widely studied chemical process, and fault prediction and classification are critical tasks for ensuring its smooth operation and safety.\n",
    "\n",
    "The notebook includes:\n",
    "- Data preprocessing and feature engineering to prepare the dataset for modeling.\n",
    "- Implementation of machine learning algorithms: random forest for real time prediction and LSTM for classification.\n",
    "- Evaluation of model performance using relevant metrics like accuracy, precision, recall, and F1-score.\n",
    "- Fine-tuning of models through hyperparameter optimization to enhance predictive capabilities.\n",
    "- Visualization of results and insights into the behavior of different models.\n",
    "\n",
    "This notebook serves as a practical example for engineers, researchers, and practitioners interested in applying machine learning techniques to fault prediction and classification in complex industrial processes like the Tennessee Eastman Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dependencies must be installed and ran for the proper function of the code base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: gdown in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from gdown) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->gdown) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kamin\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas numpy matplotlib seaborn scikit-learn tensorflow gdown joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gdown\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, Dense, Input\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool, cpu_count, Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing and pre-processing\n",
    "\n",
    "The relevant data files for the model can be obtained by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1BxF3tVenKz6jgF-kXH71mYTtpYAD8VPA\n",
      "From (redirected): https://drive.google.com/uc?id=1BxF3tVenKz6jgF-kXH71mYTtpYAD8VPA&confirm=t&uuid=c5677f6f-c4e3-4da9-85ee-1b08d941fda0\n",
      "To: d:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\MECE788Group1\\main\\temp_downloads\\faulty_training.csv\n",
      "100%|██████████| 1.87G/1.87G [00:49<00:00, 38.0MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fdgoJ5ZEuWNUTkY1w6aNhzyYDL7bDbeo\n",
      "To: d:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\MECE788Group1\\main\\temp_downloads\\faultfree_training.csv\n",
      "100%|██████████| 93.6M/93.6M [00:02<00:00, 39.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1nGSmLzCx3Ku2999ZcfRt6L4Fh2m1Aaly\n",
      "From (redirected): https://drive.google.com/uc?id=1nGSmLzCx3Ku2999ZcfRt6L4Fh2m1Aaly&confirm=t&uuid=08d07185-e97b-4aee-b48a-b715ee5cf5cd\n",
      "To: d:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\MECE788Group1\\main\\temp_downloads\\faulty_testing.csv\n",
      "100%|██████████| 3.60G/3.60G [01:34<00:00, 38.1MB/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1x_2gXR0Cnf2AA72q6PUIIipl_XQQMihI\n",
      "From (redirected): https://drive.google.com/uc?id=1x_2gXR0Cnf2AA72q6PUIIipl_XQQMihI&confirm=t&uuid=685a8b6b-b497-4074-8095-b1a025199a3b\n",
      "To: d:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\MECE788Group1\\main\\temp_downloads\\faultfree_testing.csv\n",
      "100%|██████████| 180M/180M [00:04<00:00, 39.2MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv(r'D:\\\\OneDrive - Alberta Innovates\\\\school\\\\machine learning - 788\\x0cinal project\\\\TEP_Faulty_Training.csv', on_bad_lines='skip')\\ndf2 = pd.read_csv(r'D:\\\\OneDrive - Alberta Innovates\\\\school\\\\machine learning - 788\\x0cinal project\\\\TEP_FaultFree_Training.csv', on_bad_lines='skip')\\ndf_test = pd.read_csv(r'D:\\\\OneDrive - Alberta Innovates\\\\school\\\\machine learning - 788\\x0cinal project\\\\TEP_Faulty_Testing.csv', on_bad_lines='skip')\\ndf2_test = pd.read_csv(r'D:\\\\OneDrive - Alberta Innovates\\\\school\\\\machine learning - 788\\x0cinal project\\\\TEP_FaultFree_Testing.csv', on_bad_lines='skip')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a temporary directory to store downloaded files\n",
    "temp_dir = 'temp_downloads'\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "\n",
    "# List of file URLs\n",
    "file_urls = [\n",
    "    'https://drive.google.com/file/d/1BxF3tVenKz6jgF-kXH71mYTtpYAD8VPA/view?usp=sharing',\n",
    "    'https://drive.google.com/file/d/1fdgoJ5ZEuWNUTkY1w6aNhzyYDL7bDbeo/view?usp=sharing',\n",
    "    'https://drive.google.com/file/d/1nGSmLzCx3Ku2999ZcfRt6L4Fh2m1Aaly/view?usp=sharing',\n",
    "    'https://drive.google.com/file/d/1x_2gXR0Cnf2AA72q6PUIIipl_XQQMihI/view?usp=sharing'\n",
    "]\n",
    "\n",
    "# List of output file names\n",
    "output_files = [\n",
    "    'faulty_training.csv',\n",
    "    'faultfree_training.csv',\n",
    "    'faulty_testing.csv',\n",
    "    'faultfree_testing.csv'\n",
    "]\n",
    "\n",
    "# Reading each file directly into a DataFrame\n",
    "dataframes = []\n",
    "for url, output_file in zip(file_urls, output_files):\n",
    "    # Downloading the file into the temporary directory\n",
    "    url_parts = url.split('/')\n",
    "    file_id = url_parts[-2].split('=')[-1]\n",
    "    url_direct = f'https://drive.google.com/uc?id={file_id}'\n",
    "    output_path = os.path.join(temp_dir, output_file)\n",
    "    gdown.download(url_direct, output_path, quiet=False)\n",
    "\n",
    "    # Reading the CSV file into a DataFrame\n",
    "    df = pd.read_csv(output_path, on_bad_lines='skip')\n",
    "\n",
    "    # Appending the DataFrame to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Unpacking the list of DataFrames\n",
    "faulty_training, faultfree_training, faulty_testing, faultfree_testing = dataframes\n",
    "\n",
    "# Clean up temporary directory\n",
    "import shutil\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "'''\n",
    "df = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\TEP_Faulty_Training.csv', on_bad_lines='skip')\n",
    "df2 = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\TEP_FaultFree_Training.csv', on_bad_lines='skip')\n",
    "df_test = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\TEP_Faulty_Testing.csv', on_bad_lines='skip')\n",
    "df2_test = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\TEP_FaultFree_Testing.csv', on_bad_lines='skip')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train = faulty_training.copy()\n",
    "ff_train = faultfree_training.copy()\n",
    "f_test = faulty_testing.copy()\n",
    "ff_test = faultfree_testing.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As faults are introduced every **hour** for the faulty training data set, and every **8 hours** for the faulty testing data set, a faultOccurence variable is added, and the faultNumber labelling is corrected for mislabelled data for the first 20 and 160 points, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_set_fault_occurrence(df, num_first, num_last=None):\n",
    "    '''\n",
    "    Add and set the 'faultOccurrence' column for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing fault data.\n",
    "    - num_first: Number of initial rows to label as non-faulty.\n",
    "    - num_last: Number of final rows to label as faulty (default=None, all rows are labeled as non-faulty).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with 'faultOccurrence' column added and values set for binary classification.\n",
    "    '''\n",
    "    \n",
    "    # Check if 'faultOccurrence' column already exists\n",
    "    if 'faultOccurrence' not in df.columns:\n",
    "        # Inserting the 'faultOccurrence' column filled with NaN initially to the right of 'faultNumber'\n",
    "        fault_number_index = df.columns.get_loc('faultNumber')\n",
    "        df.insert(fault_number_index + 1, 'faultOccurrence', pd.NA)\n",
    "\n",
    "    # Function to set values based on the condition\n",
    "    def set_fault_occurrence(group):\n",
    "        if num_last is not None:\n",
    "            group.iloc[:num_first, df.columns.get_loc('faultOccurrence')] = 0\n",
    "            group.iloc[-num_last:, df.columns.get_loc('faultOccurrence')] = 1\n",
    "        else:\n",
    "            group.iloc[:, df.columns.get_loc('faultOccurrence')] = 0\n",
    "        return group\n",
    "\n",
    "    # Applying the function to each group\n",
    "    df = df.groupby(df.index // (num_first + (num_last or 0))).apply(set_fault_occurrence)\n",
    "    df['faultOccurrence'] = df['faultOccurrence'].astype(int)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Add the faultOccurence column for binary classification\n",
    "f_train = add_and_set_fault_occurrence(f_train, 20, 480)\n",
    "f_test = add_and_set_fault_occurrence(f_test, 160, 800)\n",
    "ff_train = add_and_set_fault_occurrence(ff_train, len(ff_train))\n",
    "ff_test = add_and_set_fault_occurrence(ff_test, len(ff_test))\n",
    "\n",
    "# Set faultNumber to 0 for rows where faultOccurence is 0 (no fault)\n",
    "f_train.loc[f_train['faultOccurrence'] == 0, 'faultNumber'] = 0\n",
    "f_test.loc[f_test['faultOccurrence'] == 0, 'faultNumber'] = 0\n",
    "ff_train.loc[ff_train['faultOccurrence'] == 0, 'faultNumber'] = 0\n",
    "ff_test.loc[ff_test['faultOccurrence'] == 0, 'faultNumber'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the data pre-processing stage, for binary classification (using Random Forest), a stitched data set for training is generated using statistical properties of the fault free data sets provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_training = faultfree_training.mean()\n",
    "std_training = faultfree_training.std()\n",
    "avg_testing = faultfree_testing.mean()\n",
    "std_testing = faultfree_testing.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub-sample (20)\n",
    "#f_train = f_train.loc[(f_train['simulationRun']<41),:]\n",
    "#ff_train = ff_train.loc[(ff_train['simulationRun']<41),:]\n",
    "\n",
    "#f_test = f_test.loc[(f_test['simulationRun']<41),:]\n",
    "#ff_test = ff_test.loc[(ff_test['simulationRun']<41),:]\n",
    "\n",
    "#Combine Test data\n",
    "#combined_df_test = pd.concat([f_test, ff_test], ignore_index=True)\n",
    "combined_df_test = f_test\n",
    "\n",
    "#Combine Training data\n",
    "#combined_df = pd.concat([f_train, ff_train], ignore_index=True)\n",
    "combined_df = f_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline for the Random Forest model involves creating lag features with four different lags, as well as applying a  Fast Fourier Transform (FFT). Features xmeas_2, xmeas_5, xmeas_12, xmeas_14, xmeas_15, xmeas_17, xmeas_37, xmv_7, xmv_8 are the targets of this feature engineering as per the initial EDA found elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = ['xmeas_2','xmeas_5', 'xmeas_12', 'xmeas_14', 'xmeas_15', 'xmeas_17', 'xmeas_37','xmv_7', 'xmv_8']\n",
    "\n",
    "def featureengg(combined_df, features_to_plot):\n",
    "    clean_df = combined_df.drop(columns=features_to_plot).copy()\n",
    "\n",
    "    window_size = 20\n",
    "    lag_intervals = [2, 4, 6, 8]\n",
    "\n",
    "    new_columns = defaultdict(list)\n",
    "\n",
    "    for fault in clean_df['faultNumber'].unique():\n",
    "        for run in clean_df[clean_df['faultNumber'] == fault]['simulationRun'].unique():\n",
    "            mask = (clean_df['faultNumber'] == fault) & (clean_df['simulationRun'] == run)\n",
    "\n",
    "            for feature in clean_df.columns.difference(['faultNumber', 'simulationRun', 'sample']):\n",
    "                ma = clean_df.loc[mask, feature].rolling(window=window_size, min_periods=1).mean()\n",
    "                std = clean_df.loc[mask, feature].rolling(window=window_size, min_periods=1).std()\n",
    "                fft_max = clean_df.loc[mask, feature].rolling(window=window_size).apply(lambda x: np.abs(np.fft.fft(x)[-1]), raw=True)\n",
    "\n",
    "                new_columns[f'{feature}_MA'].extend(ma)\n",
    "                new_columns[f'{feature}_STD'].extend(std)\n",
    "                new_columns[f'{feature}_FFT_Max'].extend(fft_max)\n",
    "\n",
    "                for lag in lag_intervals:\n",
    "                    lag_name = f'{feature}_lag{lag}'\n",
    "                    lagged = clean_df.loc[mask, feature].shift(lag)\n",
    "                    new_columns[lag_name].extend(lagged)\n",
    "\n",
    "    new_columns_df = pd.DataFrame(new_columns, index=clean_df.index)\n",
    "    clean_df = pd.concat([clean_df, new_columns_df], axis=1)\n",
    "\n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering of Training Data\n",
    "clean_df = featureengg(combined_df, features_to_plot)\n",
    "\n",
    "#For Binary classification\n",
    "clean_df_cleaned = clean_df.dropna(axis=0)\n",
    "comb_x_df = clean_df_cleaned.drop(columns=['faultNumber','simulationRun','sample','faultOccurrence'])\n",
    "comb_y_df = clean_df_cleaned['faultOccurrence']\n",
    "\n",
    "#Feature engineering of Testing Data\n",
    "clean_df_test = featureengg(combined_df_test, features_to_plot)\n",
    "\n",
    "#For Binary classification\n",
    "clean_df_test_cleaned = clean_df_test.dropna(axis=0)\n",
    "comb_x_df_test = clean_df_test_cleaned.drop(columns=['faultNumber','simulationRun','sample','faultOccurrence'])\n",
    "comb_y_df_test = clean_df_test_cleaned['faultOccurrence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial transformer pipeline\n",
    "initial_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "])\n",
    "\n",
    "# Define the classifier\n",
    "classifier = RandomForestClassifier(n_estimators=200, max_depth=40)\n",
    "\n",
    "# Create the full pipeline with initial transformation and classifier\n",
    "full_pipeline = Pipeline([\n",
    "    ('initial_transform', initial_transformer),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "X_new = full_pipeline.fit(comb_x_df, comb_y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results for binary model\n",
    "predictions = X_new.predict(comb_x_df_test)\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual': comb_y_df_test,\n",
    "    'Predicted': predictions,\n",
    "    'Correct': comb_y_df_test == predictions,\n",
    "})\n",
    "\n",
    "\n",
    "results_df['sample'] = clean_df_test_cleaned['sample']\n",
    "results_df['simulationRun'] = clean_df_test_cleaned['simulationRun']\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "conf_matrix = confusion_matrix(results_df.loc[results_df['sample']>160,'Actual'],\n",
    "                               results_df.loc[results_df['sample']>160,'Predicted'])\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "\n",
    "accuracy = accuracy_score(results_df.loc[results_df['sample']>160,'Actual'],\n",
    "                               results_df.loc[results_df['sample']>160,'Predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "plt.show()\n",
    "anotherone = results_df.loc[results_df['sample']>160].groupby(['Actual']).agg({'Predicted':'count', 'Correct':'sum'})\n",
    "anotherone['accuracy'] = anotherone['Correct']/anotherone['Predicted']\n",
    "\n",
    "results_df.loc[results_df['sample'] < 161, 'Actual'] = 0\n",
    "conf_matrix =confusion_matrix(results_df['Actual'],\n",
    "                               results_df['Predicted'])\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(results_df['Actual'], results_df['Predicted'], average='weighted')\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(results_df['Actual'], results_df['Predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "anotherone = results_df.loc[results_df['sample']>160].groupby(['Actual']).agg({'Predicted':'count', 'Correct':'sum'})\n",
    "anotherone['accuracy'] = anotherone['Correct']/anotherone['Predicted']\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More results for binary model\n",
    "results_df.loc[results_df['sample'] < 161, 'Actual'] = 0\n",
    "conf_matrix =confusion_matrix(results_df['Actual'],\n",
    "                               results_df['Predicted'])\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(results_df['Actual'], results_df['Predicted'], average='weighted')\n",
    "print(f\"Precision: {precision}\\nRecall: {recall}\\nF1-Score: {f1}\")\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(results_df['Actual'], results_df['Predicted'])\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_df = results_df[results_df['Actual'] == results_df['Predicted']]\n",
    "filtered_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index from filtered_results_df\n",
    "filtered_index = filtered_results_df.index\n",
    "\n",
    "# Filter combined_df_test using the index\n",
    "filtered_combined_df_test = combined_df_test.loc[filtered_index]\n",
    "filtered_combined_df_test = filtered_combined_df_test.drop(columns='faultOccurrence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test sets\n",
    "train = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\train.csv', on_bad_lines='skip')\n",
    "cv = pd.read_csv(r'D:\\OneDrive - Alberta Innovates\\school\\machine learning - 788\\final project\\cv.csv', on_bad_lines='skip')\n",
    "test = filtered_combined_df_test\n",
    "\n",
    "tr = train.reset_index()\n",
    "ts = test.reset_index()\n",
    "cv_ = cv.reset_index()\n",
    "\n",
    "# Map fault numbers to a new continuous range\n",
    "#mapping = {0:0, 1:1, 2:2, 4:3, 5:4, 6:5, 7:6, 8:7, 10:8, 11:9, 12:10, 13:11, 14:12, 16:13, 17:14, 18:15, 19:16, 20:17}\n",
    "mapping = {0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, 7:7, 8:8, 9:9, 10:10, 11:11, 12:12, 13:13, 14:14, 15:15, 16:16, 17:17, 18:18, 19:19, 20:20}\n",
    "tr['mappedFaultNumber'] = tr['faultNumber'].map(mapping)\n",
    "ts['mappedFaultNumber'] = ts['faultNumber'].map(mapping)\n",
    "cv_['mappedFaultNumber'] = cv_['faultNumber'].map(mapping)\n",
    "cv_.drop(cv.columns[0], axis=1, inplace=True)\n",
    "tr.drop(cv.columns[0], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SlidingWindow(df, w, s):\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    # Find the index of the column where 'xmeas_1' is located\n",
    "    start_column_index = df.columns.get_loc('xmeas_1')\n",
    "    \n",
    "    # Find the index of the column before the last one\n",
    "    end_column_index = len(df.columns) - 1\n",
    "    \n",
    "    for i in range(0, len(df)-w+1, s):\n",
    "        # Selecting features from 'xmeas_1' to the column before the last one\n",
    "        x = np.array(df.iloc[i:i+w, start_column_index:end_column_index])  \n",
    "        \n",
    "        # Selecting the target value (mappedFaultNumber) from the last column\n",
    "        y = df.iloc[i+w-1, -1]  \n",
    "        \n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def process_data(tr, ts, cv_, SlidingWindow):\n",
    "    # Initialize window size and step size\n",
    "    w = 20\n",
    "    s = 10\n",
    "\n",
    "    # Initialize lists for storing the data\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    X_cv = []\n",
    "    Y_cv = []\n",
    "\n",
    "    # Adjust the loop for the correct number of simulation runs for training, test, and cross-validation sets\n",
    "    for dataset, max_runs_range in [(tr, range(1, 41)), (ts, range(1, 11)), (cv_, range(41, 61))]:\n",
    "        for j in range(0, 18):  # Iterating correctly from 0 to 17\n",
    "            for i in max_runs_range:  # Iterate over the correct number of simulation runs\n",
    "                df = dataset[(dataset['mappedFaultNumber'] == j) & (dataset['simulationRun'] == i)]\n",
    "                x_temp, y_temp = SlidingWindow(df, w, s)\n",
    "                if dataset is tr:\n",
    "                    X_train.extend(x_temp)\n",
    "                    Y_train.extend(y_temp)\n",
    "                elif dataset is ts:\n",
    "                    X_test.extend(x_temp)\n",
    "                    Y_test.extend(y_temp)\n",
    "                else:\n",
    "                    X_cv.extend(x_temp)\n",
    "                    Y_cv.extend(y_temp)\n",
    "\n",
    "    # Convert X and Y to numpy arrays and reshape X for LSTM\n",
    "    X_train = np.array(X_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    X_train = X_train.reshape(-1, w, X_train.shape[-1])\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    Y_test = np.array(Y_test)\n",
    "    X_test = X_test.reshape(-1, w, X_test.shape[-1])\n",
    "\n",
    "    X_cv = np.array(X_cv)\n",
    "    Y_cv = np.array(Y_cv)\n",
    "    X_cv = X_cv.reshape(-1, w, X_cv.shape[-1])\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test, X_cv, Y_cv\n",
    "\n",
    "# Call the function to process the data\n",
    "X_train, Y_train, X_test, Y_test, X_cv, Y_cv = process_data(tr, ts, cv_, SlidingWindow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of Y_train:\", Y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of Y_test:\", Y_test.shape)\n",
    "print(\"Shape of X_cv:\", X_cv.shape)\n",
    "print(\"Shape of Y_cv:\", Y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a StandardScaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Filter the DataFrame to only include data from the first simulation run\n",
    "reduced_data = tr[tr['faultNumber'] == 0]\n",
    "# Select the rows from the reduced dataset that have faultNumber equal to 0, and extract the feature values\n",
    "fault_free = reduced_data.iloc[:, 4:-1]\n",
    "# Fit the StandardScaler object to the feature values of the fault-free data\n",
    "sc.fit(fault_free)\n",
    "\n",
    "# Transform the datasets using the fitted StandardScaler\n",
    "X_train_sc = sc.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_test_sc = sc.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "X_cv_sc = sc.transform(X_cv.reshape(-1, X_cv.shape[-1])).reshape(X_cv.shape)\n",
    "\n",
    "# Create an instance of the OneHotEncoder with the 'sparse' parameter set to False\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit the OneHotEncoder to the target values Y_train\n",
    "enc.fit(Y_train.reshape(-1, 1))\n",
    "\n",
    "# Transform the target values to one-hot encoded form for train, test, and cross-validation sets\n",
    "Y_train_enc = enc.transform(Y_train.reshape(-1, 1))\n",
    "Y_test_enc = enc.transform(Y_test.reshape(-1, 1))\n",
    "Y_cv_enc = enc.transform(Y_cv.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(X_train,y_train):\n",
    "    # Define input layer\n",
    "    input_layer = Input(shape=(X_train.shape[1],X_train.shape[2]))\n",
    "\n",
    "    # Define encoder layers\n",
    "    encoded = Bidirectional(LSTM(128,activation=\"tanh\",return_sequences=True))(input_layer)\n",
    "    encoded = LSTM(128, activation=\"tanh\")(encoded)\n",
    "\n",
    "    # Define decoder layers\n",
    "    decoded = Dense(300, activation='selu')(encoded)\n",
    "    decoded = Dropout(0.5)(decoded)\n",
    "    decoded = Dense(y_train.shape[1], activation='softmax')(decoded)\n",
    "\n",
    "    # Define LSTM model\n",
    "    lstm_model = Model(inputs=input_layer, outputs=decoded)\n",
    "    # Compile LSTM  model\n",
    "    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback to monitor validation loss and stop if it doesn't improve for 5 epochs\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "# Create model\n",
    "model = LSTM_model(X_train_sc,Y_train_enc)\n",
    "model.summary()\n",
    "# Train the model with 200 epochs and batch size of 32, using the early stopping callback\n",
    "history = model.fit(X_train_sc, Y_train_enc, epochs=200, batch_size=256, validation_data=(X_cv_sc, Y_cv_enc), callbacks=[early_stop])\n",
    "\n",
    "# Plot the training history for loss and accuracy\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom class labels\n",
    "custom_labels = ['0', '1', '2', '4', '5', '6', '7', '8', '10', '11', '12', '13', '14', '16', '17', '18', '19','20']\n",
    "\n",
    "# Obtain predictions and true labels\n",
    "y_pred = enc.inverse_transform(model.predict(X_test_sc, verbose=0))\n",
    "y_true = enc.inverse_transform(Y_test_enc)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred, labels=custom_labels)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "# Print accuracy score if needed\n",
    "print(\"Bidirectional LSTM integrated with ANN (More Faults) Classification accuracy_score:\", accuracy_score(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
